{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy4Ibc1PTX3d"
      },
      "source": [
        "## Load Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hNjQa8cTX3h",
        "outputId": "b38b70ff-d500-4a5f-9e80-36441b56c6c1"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install langchain\n",
        "!pip install PyPDF2\n",
        "!pip install openai chromadb\n",
        "!pip install tiktoken\n",
        "!pip install python-pptx\n",
        "!pip install pathlib\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63LX1YudU-3C",
        "outputId": "a782e274-b56c-418b-bc58-85b79e7082fc"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "import openai\n",
        "import signal\n",
        "import datetime\n",
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "import langchain\n",
        "import PyPDF2\n",
        "from pptx import Presentation\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the working directory\n",
        "\n",
        "current_directory = os.getcwd()\n",
        "os.chdir(os.path.join(current_directory, '../TuningGPT'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvApFCVC9UHv",
        "outputId": "12c4fb22-fbfe-4555-c82c-06acd52990ca"
      },
      "outputs": [],
      "source": [
        "# Convert everything from a folder to .txt\n",
        "\n",
        "def pdf_to_txt(pdf_file_path, txt_file_path):\n",
        "    try:\n",
        "        with open(pdf_file_path, 'rb') as pdf_file:\n",
        "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "            num_pages = len(pdf_reader.pages)\n",
        "\n",
        "            with open(txt_file_path, 'w', encoding='utf-8') as txt_file:\n",
        "                for page_num in range(num_pages):\n",
        "                    page = pdf_reader.pages[page_num]\n",
        "                    txt_file.write(page.extract_text())\n",
        "\n",
        "        print(f\"Successfully converted '{pdf_file_path}' to '{txt_file_path}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while converting '{pdf_file_path}': {e}\")\n",
        "\n",
        "def pptx_to_txt(pptx_file_path, txt_file_path):\n",
        "    try:\n",
        "        prs = Presentation(pptx_file_path)\n",
        "        text_content = []\n",
        "        for slide in prs.slides:\n",
        "            for shape in slide.shapes:\n",
        "                if hasattr(shape, \"text\"):\n",
        "                    text_content.append(shape.text)\n",
        "\n",
        "        with open(txt_file_path, 'w', encoding='utf-8') as txt_file:\n",
        "            txt_file.write('\\n'.join(text_content))\n",
        "\n",
        "        print(f\"Successfully converted '{pptx_file_path}' to '{txt_file_path}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while converting '{pptx_file_path}': {e}\")\n",
        "\n",
        "def convert_non_txt_to_txt(folder_path):\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            continue  # Skip txt files\n",
        "\n",
        "        old_file_path = os.path.join(folder_path, filename)\n",
        "        new_file_path = os.path.join(folder_path, os.path.splitext(filename)[0] + \".txt\")\n",
        "\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            pdf_to_txt(old_file_path, new_file_path)\n",
        "        elif filename.endswith(\".pptx\"):\n",
        "            pptx_to_txt(old_file_path, new_file_path)\n",
        "        else:\n",
        "            print(f\"Unsupported file format: '{filename}'\")\n",
        "\n",
        "folder_path = \"../Material\"\n",
        "convert_non_txt_to_txt(folder_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6shUSxNUD7U"
      },
      "outputs": [],
      "source": [
        "# Remove empty lines from all .txt files\n",
        "\"\"\"\n",
        "from pathlib import Path\n",
        "\n",
        "def remove_empty_lines(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Filter out empty lines\n",
        "    non_empty_lines = [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "    with open(file_path, 'w') as file:\n",
        "        file.write('\\n'.join(non_empty_lines))\n",
        "\n",
        "def remove_empty_lines_from_files(directory_path):\n",
        "    path = Path(directory_path)\n",
        "    txt_files = path.glob(\"*.txt\")\n",
        "\n",
        "    for file in txt_files:\n",
        "        remove_empty_lines(file)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    directory_path = \"/../Material\"\n",
        "    remove_empty_lines_from_files(directory_path)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxtZSZYycMlX"
      },
      "outputs": [],
      "source": [
        "directory_path = '../Material'\n",
        "\n",
        "file_contents = []\n",
        "\n",
        "for filename in os.listdir(directory_path):\n",
        "    if filename.endswith('.txt'):\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        with open(file_path, 'r') as file:\n",
        "            file_contents.append(file.read())\n",
        "\n",
        "merged_content = '\\n'.join(file_contents)\n",
        "\n",
        "merged_file_path = '../Material/merge.txt'\n",
        "\n",
        "with open(merged_file_path, 'w') as merged_file:\n",
        "    merged_file.write(merged_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AupjBMcVEAR"
      },
      "source": [
        "## Fine tuning Conditional Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0DgDRNKgTtp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import constants\n",
        "\n",
        "# To use Langchain, we recommend having a .py file that contains the following line: APIKEY = 'Your_API_Key'\n",
        "os.environ[\"OPENAI_API_KEY\"] = constants.APIKEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Vsxcv5YVfC5"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader('../Material/merge.txt')\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWtqctP6dEC5"
      },
      "outputs": [],
      "source": [
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "index = VectorstoreIndexCreator().from_loaders([loader])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dlmSkIQh_PY",
        "outputId": "03812cbf-ca11-47f4-886f-e38d3842bf46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The instructor of the course is Ryan Baker.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "print(index.query('who is the instructor',llm = ChatOpenAI()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP6V9JMKCp3S",
        "outputId": "93ef38e0-6c65-4078-cdbc-8977ae5289ab"
      },
      "outputs": [],
      "source": [
        "def process_dataframe(source, file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    responses = []\n",
        "    for index, row in df.iterrows():\n",
        "        loader = TextLoader(source)\n",
        "        data = loader.load()\n",
        "        index = VectorstoreIndexCreator().from_loaders([loader])\n",
        "        prompt = row[\"prompt\"]\n",
        "        response = index.query(prompt,llm = ChatOpenAI())\n",
        "        responses.append(response)\n",
        "    return responses\n",
        "\n",
        "source = \"../Material/syllabus.txt\"\n",
        "file_path = \"../Test_NaturalQuestion.csv\"\n",
        "responses = process_dataframe(source, file_path)\n",
        "responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IQmMF58RmmX",
        "outputId": "70052ffb-ec7a-459a-b3a6-ee732a199833"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Responses successfully filled in the file.\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Make sure the number of items in the list matches the number of rows in the DataFrame\n",
        "if len(responses) != len(df):\n",
        "    print(\"Number of responses doesn't match the number of rows in the file.\")\n",
        "else:\n",
        "    # Fill in the \"response\" column\n",
        "    for i, response in enumerate(responses):\n",
        "        df.at[i, \"response\"] = response\n",
        "\n",
        "    # Save the updated DataFrame back to the CSV file\n",
        "    df.to_csv(file_path, index=False)\n",
        "\n",
        "    print(\"Responses successfully filled in the file.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "3b138a8faad971cc852f62bcf00f59ea0e31721743ea2c5a866ca26adf572e75"
    },
    "kernelspec": {
      "display_name": "Python 3.7.3 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
