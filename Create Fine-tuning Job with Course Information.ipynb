{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jy4Ibc1PTX3d"
   },
   "source": [
    "# Fine-tuning Conditional Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T04:54:44.608663Z",
     "start_time": "2023-07-20T04:54:42.059097Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2hNjQa8cTX3h",
    "outputId": "b0184617-b3cd-4caf-beb7-418cc1cb96df"
   },
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install langchain\n",
    "!pip install PyPDF2\n",
    "!pip install openai chromadb\n",
    "!pip install tiktoken\n",
    "!pip install python-pptx\n",
    "!pip install pathlib\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T04:54:50.688984Z",
     "start_time": "2023-07-20T04:54:47.392817Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import openai\n",
    "import signal\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import langchain\n",
    "import PyPDF2\n",
    "from pptx import Presentation\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "os.chdir(os.path.join(current_directory, '../TuningGPT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert everything from a folder to .txt\n",
    "\n",
    "def pdf_to_txt(pdf_file_path, txt_file_path):\n",
    "    try:\n",
    "        with open(pdf_file_path, 'rb') as pdf_file:\n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "\n",
    "            with open(txt_file_path, 'w', encoding='utf-8') as txt_file:\n",
    "                for page_num in range(num_pages):\n",
    "                    page = pdf_reader.pages[page_num]\n",
    "                    txt_file.write(page.extract_text())\n",
    "\n",
    "        print(f\"Successfully converted '{pdf_file_path}' to '{txt_file_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while converting '{pdf_file_path}': {e}\")\n",
    "\n",
    "def pptx_to_txt(pptx_file_path, txt_file_path):\n",
    "    try:\n",
    "        prs = Presentation(pptx_file_path)\n",
    "        text_content = []\n",
    "        for slide in prs.slides:\n",
    "            for shape in slide.shapes:\n",
    "                if hasattr(shape, \"text\"):\n",
    "                    text_content.append(shape.text)\n",
    "\n",
    "        with open(txt_file_path, 'w', encoding='utf-8') as txt_file:\n",
    "            txt_file.write('\\n'.join(text_content))\n",
    "\n",
    "        print(f\"Successfully converted '{pptx_file_path}' to '{txt_file_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while converting '{pptx_file_path}': {e}\")\n",
    "\n",
    "def convert_non_txt_to_txt(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            continue  # Skip txt files\n",
    "\n",
    "        old_file_path = os.path.join(folder_path, filename)\n",
    "        new_file_path = os.path.join(folder_path, os.path.splitext(filename)[0] + \".txt\")\n",
    "\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_to_txt(old_file_path, new_file_path)\n",
    "        elif filename.endswith(\".pptx\"):\n",
    "            pptx_to_txt(old_file_path, new_file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file format: '{filename}'\")\n",
    "\n",
    "folder_path = \"../Material\"\n",
    "convert_non_txt_to_txt(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty lines from all .txt files\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def remove_empty_lines(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Filter out empty lines\n",
    "    non_empty_lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write('\\n'.join(non_empty_lines))\n",
    "\n",
    "def remove_empty_lines_from_files(directory_path):\n",
    "    path = Path(directory_path)\n",
    "    txt_files = path.glob(\"*.txt\")\n",
    "\n",
    "    for file in txt_files:\n",
    "        remove_empty_lines(file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory_path = \"../Material\"\n",
    "    remove_empty_lines_from_files(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training data (leaving prompt blank and just filling in completion with around 1,000 tokens. Token definition can be found here: https://platform.openai.com/tokenizer)\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "def count_tokens(text):\n",
    "    # Load the GPT-2 tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    # Tokenize the text and return the token count\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    return len(tokens)\n",
    "\n",
    "def chunk_text(text, max_tokens):\n",
    "    chunks = []\n",
    "    current_chunk = ''\n",
    "    current_token_count = 0\n",
    "\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "        if current_token_count + sentence_tokens <= max_tokens:\n",
    "            current_chunk += sentence\n",
    "            current_token_count += sentence_tokens\n",
    "        else:\n",
    "            chunks.append((current_chunk, current_token_count))\n",
    "            current_chunk = sentence\n",
    "            current_token_count = sentence_tokens\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append((current_chunk, current_token_count))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def read_txt_files_and_create_csv(txt_folder_path, csv_file_path, max_tokens):\n",
    "    with open(csv_file_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['prompt', 'completion', 'token_count'])\n",
    "\n",
    "        for filename in os.listdir(txt_folder_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                txt_file_path = os.path.join(txt_folder_path, filename)\n",
    "                with open(txt_file_path, 'r', encoding='utf-8') as txt_file:\n",
    "                    text_content = txt_file.read()\n",
    "                    chunks = chunk_text(text_content, max_tokens)\n",
    "                    for chunk, token_count in chunks:\n",
    "                        csv_writer.writerow(['', chunk, token_count])\n",
    "\n",
    "txt_folder_path = \"../Material\"\n",
    "csv_file_path = \"training_data.csv\"\n",
    "max_tokens_per_row = 999\n",
    "\n",
    "read_txt_files_and_create_csv(txt_folder_path, csv_file_path, max_tokens_per_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'token_count' column\n",
    "csv_file_path = \"training_data.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "df = df.drop(columns=['token_count'])\n",
    "\n",
    "output_file_path = \"training_data.csv\"\n",
    "df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# If you do not want to inspect token count, you may:\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "\n",
    "def count_tokens(text):\n",
    "    # Load the GPT-2 tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    # Tokenize the text and return the token count\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    return len(tokens)\n",
    "\n",
    "def chunk_text(text, max_tokens):\n",
    "    chunks = []\n",
    "    current_chunk = ''\n",
    "    current_token_count = 0\n",
    "\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "        if current_token_count + sentence_tokens <= max_tokens:\n",
    "            current_chunk += sentence\n",
    "            current_token_count += sentence_tokens\n",
    "        else:\n",
    "            chunks.append((current_chunk, current_token_count))\n",
    "            current_chunk = sentence\n",
    "            current_token_count = sentence_tokens\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append((current_chunk, current_token_count))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def read_txt_files_and_create_csv(txt_folder_path, csv_file_path, max_tokens):\n",
    "    with open(csv_file_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['prompt', 'completion'])\n",
    "\n",
    "        for filename in os.listdir(txt_folder_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                txt_file_path = os.path.join(txt_folder_path, filename)\n",
    "                with open(txt_file_path, 'r', encoding='utf-8') as txt_file:\n",
    "                    text_content = txt_file.read()\n",
    "                    chunks = chunk_text(text_content, max_tokens)\n",
    "                    for chunk, _ in chunks:  # We don't need the token_count here.\n",
    "                        csv_writer.writerow(['', chunk])\n",
    "\n",
    "txt_folder_path = \"../Material\"\n",
    "csv_file_path = \"training_data.csv\"\n",
    "max_tokens_per_row = 999\n",
    "\n",
    "read_txt_files_and_create_csv(txt_folder_path, csv_file_path, max_tokens_per_row)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Conditional Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T04:54:58.924418Z",
     "start_time": "2023-07-20T04:54:58.915972Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Directly assign your API key if you prefer not to use a .txt file\n",
    "default_api_key = \"<your_api_key>\"\n",
    "\n",
    "# Or, specify the filename for the API key configuration\n",
    "config_filename = \"<api_key_file>.txt\"\n",
    "\n",
    "# Check if the <api_key_file>.txt file exists in the current directory\n",
    "if os.path.isfile(config_filename):\n",
    "    with open(config_filename, 'r') as file:\n",
    "        api_key = file.readline().strip().split('=')[1]\n",
    "else:\n",
    "    # Use the default API key if the file doesn't exist\n",
    "    api_key = default_api_key\n",
    "\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T04:55:02.177579Z",
     "start_time": "2023-07-20T04:55:02.139597Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HbouxEafVdad",
    "outputId": "b4005c3a-714a-40e0-c1fb-8f79b2eb1ab5"
   },
   "outputs": [],
   "source": [
    "# Prepare data as block of content\n",
    "\n",
    "training_data = 'training_data.csv'\n",
    "\n",
    "def prepare_data(csv_file, jsonl_file):\n",
    "    training_data = []\n",
    "\n",
    "    with open(csv_file, 'r', encoding='utf-8-sig') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            prompt = row['prompt']\n",
    "            if not prompt.endswith('?'):\n",
    "                prompt += '?'  # Add question mark if missing\n",
    "            prompt = prompt + '->'\n",
    "            completion = ' ' + row['completion']\n",
    "            if not completion.endswith('.'):\n",
    "                completion += '.'  # Add period if missing\n",
    "            completion += '\\n'\n",
    "            entry = {'prompt': prompt,\n",
    "\t\t\t\t\t           'completion': completion}\n",
    "            training_data.append(entry)\n",
    "\n",
    "    with open(jsonl_file, 'w') as jsonlfile:\n",
    "        for entry in training_data:\n",
    "            print (entry)\n",
    "            json.dump(entry, jsonlfile)\n",
    "            jsonlfile.write('\\n')\n",
    "\n",
    "\n",
    "prepare_data(training_data, 'training_data.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cOh4hao6WWLr",
    "outputId": "da10510f-76bc-49a1-99cf-70b65023ae2b"
   },
   "outputs": [],
   "source": [
    "!openai tools fine_tunes.prepare_data -f \"training_data.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T04:35:41.092631Z",
     "start_time": "2023-07-20T04:35:40.157609Z"
    },
    "id": "zuP2-34uVs1q"
   },
   "outputs": [],
   "source": [
    "training_file_id = openai.File.create(\n",
    "  file=open(\"training_data.jsonl\", \"rb\"),\n",
    "  purpose='fine-tune'\n",
    ")[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Sending a Fine-tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FhGWrfaUaT0G",
    "outputId": "7c69efad-a799-4ee9-e0b2-1e78ff15d4ec"
   },
   "outputs": [],
   "source": [
    "create_args = {\n",
    "\t\"training_file\": training_file_id,\n",
    "\t\"model\": \"davinci\",\n",
    "\t\"n_epochs\": 15,\n",
    "\t\"batch_size\": 3,\n",
    "\t\"learning_rate_multiplier\": 0.3\n",
    "}\n",
    "\n",
    "response = openai.FineTune.create(**create_args)\n",
    "job_id = response[\"id\"]\n",
    "status = response[\"status\"]\n",
    "\n",
    "print(f'Fine-tunning model with jobID: {job_id}.')\n",
    "print(f\"Training Response: {response}\")\n",
    "print(f\"Training Status: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4jwJsfqpaZWr",
    "outputId": "51c37b74-839e-4fb7-b0b4-5822e094cff1"
   },
   "outputs": [],
   "source": [
    "import signal\n",
    "import datetime\n",
    "\n",
    "def signal_handler(sig, frame):\n",
    "\tstatus = openai.FineTune.retrieve(job_id).status\n",
    "\tprint(f\"Stream interrupted. Job is still {status}.\")\n",
    "\treturn\n",
    "\n",
    "print(f'Streaming events for the fine-tuning job: {job_id}')\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "events = openai.FineTune.stream_events(job_id)\n",
    "try:\n",
    "  for event in events:\n",
    "    print(f'{datetime.datetime.fromtimestamp(event[\"created_at\"])} {event[\"message\"]}')\n",
    "except Exception:\n",
    "  print(\"Stream interrupted (client disconnected).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T04:41:58.883727Z",
     "start_time": "2023-07-20T04:41:58.552670Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "uJ5i0yCjeP7r",
    "outputId": "33545935-3e6d-4846-c702-9e2e93a9ff18"
   },
   "outputs": [],
   "source": [
    "# Check fine-tuning Status\n",
    "\n",
    "import time\n",
    "\n",
    "status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "if status not in [\"succeeded\", \"failed\"]:\n",
    "  print(f'Job not in terminal status: {status}. Waiting.')\n",
    "  while status not in [\"succeeded\", \"failed\"]:\n",
    "    time.sleep(2)\n",
    "    status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "    print(f'Status: {status}')\n",
    "else:\n",
    "  print(f'Finetune job {job_id} finished with status: {status}')\n",
    "\n",
    "\"\"\"\n",
    "print('Checking other finetune jobs in the subscription.')\n",
    "result = openai.FineTune.list()\n",
    "print(f'Found {len(result.data)} finetune jobs.')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T04:42:18.883255Z",
     "start_time": "2023-07-20T04:42:18.750713Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "eG65uSHSgWpe",
    "outputId": "bad8440f-55bb-4063-e6de-68e1418594aa"
   },
   "outputs": [],
   "source": [
    "# Retrieve fine-tunning job information\n",
    " \n",
    "openai.FineTune.retrieve(id=job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in job_id manually if session expires\n",
    "#job_id = 'ft-LTqFIfBcnJPH7QWQ6AbCnAhb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T04:42:24.683538Z",
     "start_time": "2023-07-20T04:42:24.443821Z"
    },
    "id": "N4WweZUGdOSe"
   },
   "outputs": [],
   "source": [
    "fine_tuned_model = openai.FineTune.retrieve(id=job_id)[\"fine_tuned_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternatively, fill in model manually if it is obtained from Postman\n",
    "#fine_tuned_model = \"<fine_tunned_model_id>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending a Prompt to a Selected Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T04:44:18.900415Z",
     "start_time": "2023-07-20T04:44:17.142961Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nt4mGJpHcia9",
    "outputId": "3fc91b65-3784-4648-9b65-f941edd37da4"
   },
   "outputs": [],
   "source": [
    "new_prompt = \"What is the name of the course?\"\n",
    "answer = openai.Completion.create(\n",
    "  model=fine_tuned_model,\n",
    "  prompt=new_prompt\n",
    ")\n",
    "\n",
    "print(answer['choices'][0]['text'])\n",
    "\n",
    "new_prompt = \"What are the grading criteria?\"\n",
    "answer = openai.Completion.create(\n",
    "  model=fine_tuned_model,\n",
    "  prompt=new_prompt\n",
    ")\n",
    "\n",
    "print(answer['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(source, file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    responses = []\n",
    "    for index, row in df.iterrows():\n",
    "        prompt = row[\"prompt\"]\n",
    "\n",
    "        answer = openai.Completion.create(\n",
    "          model=fine_tuned_model,\n",
    "          prompt=prompt\n",
    "        )\n",
    "\n",
    "        response = answer['choices'][0]['text']\n",
    "        responses.append(response)\n",
    "    return responses\n",
    "\n",
    "source = \"../Material/merge.txt\"\n",
    "file_path = \"Test_NaturalQuestion.csv\"\n",
    "responses = process_dataframe(source, file_path)\n",
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Make sure the number of items in the list matches the number of rows in the DataFrame\n",
    "if len(responses) != len(df):\n",
    "    print(\"Number of responses doesn't match the number of rows in the file.\")\n",
    "else:\n",
    "    # Fill in the \"response\" column\n",
    "    for i, response in enumerate(responses):\n",
    "        df.at[i, \"response\"] = response\n",
    "\n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "    print(\"Responses successfully filled in the file.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "3b138a8faad971cc852f62bcf00f59ea0e31721743ea2c5a866ca26adf572e75"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
